"""
Agente de Resposta AutomÃ¡tica.

ResponsÃ¡vel por gerar respostas personalizadas usando LLM (Gemini)
com base no contexto completo da avaliaÃ§Ã£o analisada.
"""

import os
from typing import Optional, Dict, Any
import google.generativeai as genai
from dotenv import load_dotenv


class ResponseAgent:
    """
    Agente especializado em geraÃ§Ã£o de respostas automÃ¡ticas.
    
    Utiliza o modelo Gemini 1.5 Flash para criar respostas empÃ¡ticas
    e contextualizadas para avaliaÃ§Ãµes de clientes.
    """
    
    def __init__(self):
        """
        Inicializa o agente de resposta e configura a API do Gemini.
        """
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        
        if api_key:
            genai.configure(api_key=api_key)
            
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 300,
            }
            
            # ConfiguraÃ§Ãµes de seguranÃ§a mais permissivas
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            self.model = genai.GenerativeModel(
                model_name="gemini-2.0-flash",
                generation_config=generation_config,
                safety_settings=safety_settings
            )
        else:
            self.model = None

    def generate_reply(
        self, 
        text: str, 
        sentiment: str, 
        validation_result: Dict[str, Any],
        action: str
    ) -> str:
        """
        Gera uma resposta automÃ¡tica para a avaliaÃ§Ã£o.
        
        Args:
            text: Texto original da avaliaÃ§Ã£o
            sentiment: Sentimento detectado (Positivo/Neutro/Negativo)
            validation_result: Resultado do ValidationAgent contendo:
                - confianca: Probabilidade da classe predita
                - status: Status de validaÃ§Ã£o
                - requer_revisao_humana: Boolean
            action: AÃ§Ã£o sugerida pelo ActionAgent
            
        Returns:
            Resposta gerada pelo LLM ou mensagem de erro
        """
        if not self.model:
            return self._generate_fallback_response(sentiment, validation_result)
        
        confianca = validation_result.get('confianca', 0.5)
        status = validation_result.get('status', 'DESCONHECIDO')
        requer_revisao = validation_result.get('requer_revisao_humana', False)
        
        # InstruÃ§Ãµes adicionais baseadas na confianÃ§a
        confidence_instruction = ""
        if requer_revisao:
            confidence_instruction = "- IMPORTANTE: A confianÃ§a da IA Ã© baixa. Mencione sutilmente que um especialista revisarÃ¡ o caso se necessÃ¡rio."
        elif confianca < 0.7:
            confidence_instruction = "- A confianÃ§a Ã© moderada. Seja um pouco mais cauteloso na resposta."
        
        prompt = f"""VocÃª Ã© um atendente de e-commerce profissional e amigÃ¡vel.
Responda Ã  avaliaÃ§Ã£o do cliente de forma empÃ¡tica e calorosa.

AvaliaÃ§Ã£o: "{text}"
Sentimento detectado: {sentiment}
ConfianÃ§a da anÃ¡lise: {confianca:.0%}
Status de validaÃ§Ã£o: {status}
AÃ§Ã£o sugerida: {action}

Requisitos:
- Tom empÃ¡tico, caloroso e profissional
- Use emojis apropriados para deixar a resposta mais amigÃ¡vel
- Se negativo: peÃ§a desculpas sinceramente, mostre empatia e ofereÃ§a soluÃ§Ã£o clara
- Se positivo: agradeÃ§a com entusiasmo e reforce o relacionamento
- Se neutro: agradeÃ§a o feedback e mostre abertura para melhorias
{confidence_instruction}

Resposta:"""
        
        try:
            response = self.model.generate_content([prompt])
            
            # Verificar se a resposta foi bloqueada por seguranÃ§a
            if not response.candidates or not response.candidates[0].content.parts:
                return self._generate_fallback_response(sentiment, validation_result)
            
            return response.text.strip()
        except Exception as e:
            # Em caso de erro, retornar resposta padrÃ£o
            return self._generate_fallback_response(sentiment, validation_result)
    
    def _generate_fallback_response(self, sentiment: str, validation_result: Dict[str, Any]) -> str:
        """
        Gera resposta padrÃ£o quando o LLM falha.
        
        Args:
            sentiment: Sentimento detectado
            validation_result: Resultado da validaÃ§Ã£o
            
        Returns:
            Resposta padrÃ£o apropriada
        """
        requer_revisao = validation_result.get('requer_revisao_humana', False)
        
        if requer_revisao:
            return "Obrigado pelo seu feedback! Um membro da nossa equipe irÃ¡ analisar sua avaliaÃ§Ã£o e entrar em contato em breve para melhor atendÃª-lo."
        
        if sentiment == "Positivo":
            return "Muito obrigado pelo seu feedback positivo! ğŸ˜Š Ficamos felizes em saber que vocÃª teve uma boa experiÃªncia. Conte sempre conosco!"
        elif sentiment == "Negativo":
            return "Lamentamos muito pela sua experiÃªncia negativa. ğŸ˜” Pedimos sinceras desculpas e vamos trabalhar para resolver isso. Por favor, entre em contato com nosso suporte para que possamos ajudÃ¡-lo."
        else:  # Neutro
            return "Obrigado pelo seu feedback! ğŸ“ Valorizamos sua opiniÃ£o e estamos sempre buscando melhorar. Se tiver alguma sugestÃ£o, estamos Ã  disposiÃ§Ã£o!"
